{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual implementation of Naive Bayes for a Spam filter\n",
    "\n",
    "In this project, we're going to manually implement Naive Bayes algorithm for educational purpose to build a SMS spam filter. Our goal is to write a program that classifies new messages with an accuracy greater than 90% — so we expect that more than 90% of the new messages will be classified correctly as spam or ham (non-spam).\n",
    "\n",
    "To train the algorithm, we'll use a dataset of 5,572 labeled SMS messages. The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). The data collection process is described in more details on [this page](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition).\n",
    "\n",
    "\n",
    "## **Background**\n",
    "\n",
    "**Naive Bayes classifiers** are part of the simple \"probabilistic classifiers\" algorithms family based on applying Bayes theorem with 'naive' independence assumptions between the features (hence it's name). Bayes' theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n",
    "\n",
    "Mathematically, we can express this relationship as:\n",
    "$P(B|E) = \\frac{P(B) \\cdot P(E|B)}{P(E)}$, \n",
    "with $P$ standing for probability, $B$ for belief and $E$ for evidence. \\\n",
    "$P(B)$ is the probability that $B$ is true, and $P(E)$ is the probability that $E$ is true. \\\n",
    "$P(B|E)$ means the probability of $B$ if $E$ is true, and $P(E|B)$ is the probability of $E$ if $B$ is true.\n",
    "\n",
    "**Naive Bayes algorithm** is a simple multinomial application of Bayes Theorem based on the idea that if the probability of a belief when applied with evidence is greater than the probability of the non-belief when applied with same evidence then the belief is \"more true\" than the \"non-belief\" or vice versa.\n",
    "\n",
    "Mathematically, we want to compute: $P(Spam | e_1,e_2,...,e_n)$ and $P(\\overline{Spam} | e_1,e_2, ..., e_n)$ where: $Spam$ is the belief and $e_1, e_2, ....e_n$ are all the evidences applied to the belief (in our case, words).\n",
    "\n",
    "\n",
    "## Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sms = pd.read_csv('SMSSpamCollection.txt', sep='\\t', header=None, names=['Label', 'SMS'])\n",
    "\n",
    "print(sms.shape)\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we see that about 87% of the messages are ham, and the remaining 13% are spam. This sample looks representative, since in practice most messages that people receive are ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_count = sms['Label'].value_counts(normalize=True)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.bar(labels_count.index, labels_count, alpha=0.7, width=0.6, edgecolor='black', linewidth=1.2, color='#3caea3')\n",
    "plt.title('Message type', fontsize=15, fontweight='bold', position=(0.25, 1.0+0.05))\n",
    "plt.yticks([])\n",
    "plt.ylim(0,0.95)\n",
    "for i in labels_count.index:\n",
    "    plt.annotate(f\"{labels_count[i]*100:.0f}%\", \n",
    "                 xy=(i, labels_count[i] + 0.03),\n",
    "                 va = 'center', ha='center',fontweight='bold', color='#383838'\n",
    "                )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Set\n",
    "\n",
    "We're now going to split our dataset into a training and a test set, where the training set accounts for 80% of the data, and the test set for the remaining 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4458, 2)\n",
      "(1114, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomize the dataset\n",
    "sms_randomized = sms.sample(frac=1, random_state=1)\n",
    "\n",
    "# Calculate index for split\n",
    "split_index = round(len(sms_randomized) * 0.8)\n",
    "\n",
    "# Training/Test split\n",
    "train = sms_randomized[:split_index].reset_index(drop=True)\n",
    "test = sms_randomized[split_index:].reset_index(drop=True)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the percentage of spam and ham messages in both sets.  \n",
    "They should be close to what we have in the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_count = train['Label'].value_counts(normalize=True)\n",
    "test_count = test['Label'].value_counts(normalize=True)\n",
    "\n",
    "fig, axs  = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "for df, ax, title in zip([train_count, test_count], axs, ['Train message type', 'Test message type']):\n",
    "    ax.bar(df.index, df, alpha=0.7, width=0.6, edgecolor='black', linewidth=1.2, color='#3caea3')\n",
    "    ax.set_title(title, fontsize=15, fontweight='bold', position=(0.25, 1.0+0.05))\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylim(0,0.95)\n",
    "    for i in df.index:\n",
    "        ax.annotate(f\"{df[i]*100:.0f}%\", \n",
    "                     xy=(i, df[i] + 0.03),\n",
    "                     va = 'center', ha='center',fontweight='bold', color='#383838'\n",
    "                    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look good! We'll now move on to cleaning the dataset.\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "To calculate all the probabilities required by the algorithm, we'll first need to perform a bit of data cleaning to bring the data in a format that will allow us to extract easily all the information we need.\n",
    "\n",
    "Essentially, we want to bring data to this format:\n",
    "\n",
    "![img](https://dq-content.s3.amazonaws.com/433/cpgp_dataset_3.png)\n",
    "\n",
    "\n",
    "### Letter Case and Punctuation\n",
    "\n",
    "We'll begin with removing all the punctuation and bringing every letter to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yep, by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes, princess. Are you going to make me moan?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Havent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I forgot 2 ask ü all smth.. There's a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       Yep, by the pretty sculpture\n",
       "1   ham      Yes, princess. Are you going to make me moan?\n",
       "2   ham                         Welp apparently he retired\n",
       "3   ham                                            Havent.\n",
       "4   ham  I forgot 2 ask ü all smth.. There's a card on ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before cleaning\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>yep  by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>yes  princess  are you going to make me moan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>havent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>i forgot 2 ask ü all smth   there s a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       yep  by the pretty sculpture\n",
       "1   ham      yes  princess  are you going to make me moan \n",
       "2   ham                         welp apparently he retired\n",
       "3   ham                                            havent \n",
       "4   ham  i forgot 2 ask ü all smth   there s a card on ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After cleaning\n",
    "train['SMS'] = (train['SMS']\n",
    "                .str.replace('\\W', ' ', regex=True)\n",
    "                .str.lower())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Final Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>yep</th>\n",
       "      <th>by</th>\n",
       "      <th>the</th>\n",
       "      <th>pretty</th>\n",
       "      <th>sculpture</th>\n",
       "      <th>yes</th>\n",
       "      <th>princess</th>\n",
       "      <th>are</th>\n",
       "      <th>...</th>\n",
       "      <th>beauty</th>\n",
       "      <th>hides</th>\n",
       "      <th>secrets</th>\n",
       "      <th>n8</th>\n",
       "      <th>jewelry</th>\n",
       "      <th>related</th>\n",
       "      <th>trade</th>\n",
       "      <th>arul</th>\n",
       "      <th>bx526</th>\n",
       "      <th>wherre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[havent]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>ham</td>\n",
       "      <td>[sorry, i, ll, call, later, in, meeting, any, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>ham</td>\n",
       "      <td>[babe, i, fucking, love, you, too, you, know, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>spam</td>\n",
       "      <td>[u, ve, been, selected, to, stay, in, 1, of, 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>ham</td>\n",
       "      <td>[hello, my, boytoy, geeee, i, miss, you, alrea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4457</th>\n",
       "      <td>ham</td>\n",
       "      <td>[wherre, s, my, boytoy]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4458 rows × 7785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS  yep  by  the  \\\n",
       "0      ham                  [yep, by, the, pretty, sculpture]    1   1    1   \n",
       "1      ham  [yes, princess, are, you, going, to, make, me,...    0   0    0   \n",
       "2      ham                    [welp, apparently, he, retired]    0   0    0   \n",
       "3      ham                                           [havent]    0   0    0   \n",
       "4      ham  [i, forgot, 2, ask, ü, all, smth, there, s, a,...    0   0    0   \n",
       "...    ...                                                ...  ...  ..  ...   \n",
       "4453   ham  [sorry, i, ll, call, later, in, meeting, any, ...    0   0    0   \n",
       "4454   ham  [babe, i, fucking, love, you, too, you, know, ...    0   0    0   \n",
       "4455  spam  [u, ve, been, selected, to, stay, in, 1, of, 2...    0   0    0   \n",
       "4456   ham  [hello, my, boytoy, geeee, i, miss, you, alrea...    0   0    0   \n",
       "4457   ham                            [wherre, s, my, boytoy]    0   0    0   \n",
       "\n",
       "      pretty  sculpture  yes  princess  are  ...  beauty  hides  secrets  n8  \\\n",
       "0          1          1    0         0    0  ...       0      0        0   0   \n",
       "1          0          0    1         1    1  ...       0      0        0   0   \n",
       "2          0          0    0         0    0  ...       0      0        0   0   \n",
       "3          0          0    0         0    0  ...       0      0        0   0   \n",
       "4          0          0    0         0    0  ...       0      0        0   0   \n",
       "...      ...        ...  ...       ...  ...  ...     ...    ...      ...  ..   \n",
       "4453       0          0    0         0    0  ...       0      0        0   0   \n",
       "4454       0          0    0         0    0  ...       0      0        0   0   \n",
       "4455       0          0    0         0    0  ...       0      0        0   0   \n",
       "4456       0          0    0         0    0  ...       0      0        0   0   \n",
       "4457       0          0    0         0    0  ...       0      0        0   0   \n",
       "\n",
       "      jewelry  related  trade  arul  bx526  wherre  \n",
       "0           0        0      0     0      0       0  \n",
       "1           0        0      0     0      0       0  \n",
       "2           0        0      0     0      0       0  \n",
       "3           0        0      0     0      0       0  \n",
       "4           0        0      0     0      0       0  \n",
       "...       ...      ...    ...   ...    ...     ...  \n",
       "4453        0        1      1     1      0       0  \n",
       "4454        0        0      0     0      0       0  \n",
       "4455        0        0      0     0      1       0  \n",
       "4456        0        0      0     0      0       0  \n",
       "4457        0        0      0     0      0       1  \n",
       "\n",
       "[4458 rows x 7785 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Split sms on words for the count\n",
    "train['SMS'] = train['SMS'].str.split()\n",
    "\n",
    "# Compute nested dictionary of word counts\n",
    "# {0: {'yep': 1, 'by': 1, ...}\n",
    "#  1: {'yes': 1, 'princess': 1, ...}\n",
    "# }\n",
    "word_dict = train['SMS'].apply(Counter).to_dict()\n",
    "\n",
    "# Create df from nested dict\n",
    "word_count = pd.DataFrame.from_dict(word_dict, orient='index')\n",
    "\n",
    "# Outer join to retrieve empty messages\n",
    "train_sms = pd.concat([train, word_count], axis=1, join='outer')\n",
    "\n",
    "# Types dictionary \n",
    "types = {col: 'int' for col in word_count.columns}\n",
    "\n",
    "# Fill NaN with 0s and cast int instead of float on words columns only\n",
    "train_clean = (train_sms\n",
    "               .fillna(0)\n",
    "               .astype(types))\n",
    "\n",
    "train_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Constants First\n",
    "\n",
    "The training set being cleaned, we can now compute the algorythm. Naive Bayes, as previously mentioned is based on two probabilities:\n",
    "\n",
    "$$\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(Ham | w_1,w_2, ..., w_n)  \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "$$\n",
    "\n",
    "\n",
    "Also, to calculate $P(w_i|Spam)$ and $P(w_i|Ham)$ inside the formulas above, we'll need to use these equations:\n",
    "\n",
    "$$\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$\n",
    "\n",
    "Where $N_{set}$ is the number of words in the set.\n",
    "\n",
    "Some of the terms in the equations above are constants. We can calculate the value of these terms once and avoid doing the computations again when a new messages comes in. Below, we'll use our training set to calculate:\n",
    "\n",
    "- $P(Spam)$ and $P(Ham)$\n",
    "- $N_{Spam}, N_{Ham}, N_{Vocabulary}$\n",
    "\n",
    "We'll also use Laplace smoothing and set $\\alpha = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating spam and ham messages first\n",
    "spam = train_clean[train_clean['Label'] == 'spam']\n",
    "ham = train_clean[train_clean['Label'] == 'ham']\n",
    "\n",
    "# P(Spam) and P(Ham)\n",
    "p_spam = len(spam) / len(train_clean)\n",
    "p_ham = len(ham) / len(train_clean)\n",
    "\n",
    "# N_spam, N_ham, N_vocabulary\n",
    "n_spam = spam.sum(axis=1).sum()\n",
    "n_ham = ham.sum(axis=1).sum()\n",
    "\n",
    "# N_Vocabulary\n",
    "vocabulary = list(train_clean.iloc[:,2:].columns)\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Parameters\n",
    "\n",
    "We can move on with calculating the parameters $P(w_i|Spam)$ and $P(w_i|Ham)$.  \n",
    "Each parameter will thus be a conditional probability value associated with each word in the vocabulary.\n",
    "\n",
    "The parameters are calculated using the formulas:\n",
    "\n",
    "$$\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(w_i|Spam)\n",
    "n_words_spam = spam.sum()[2:]\n",
    "p_words_spam = (n_words_spam+alpha)  / (n_spam + alpha*n_vocabulary)\n",
    "\n",
    "#P(w_i|Ham)\n",
    "n_words_ham = ham.sum()[2:]\n",
    "p_words_ham = (n_words_ham+alpha)  / (n_ham + alpha*n_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying A New Message\n",
    "\n",
    "Now that we have all our parameters calculated, we can start creating the spam filter. The spam filter can be understood as a function that:\n",
    "\n",
    "- Takes in as input a new message $(w_1, w_2 ..., w_n)$.\n",
    "- Clean the input according to the dataframe.\n",
    "- Calculates $P(Spam|w_1, w_2, ..., w_n)$ and $P(Ham|w_1, w_2, ..., w_n)$.\n",
    "- Compares the values of $P(Spam|w_1, w_2, ..., w_n)$ and $P(Ham|w_1, w_2, ..., w_n)$, and:\n",
    "    - If $P(Ham|w_1, w_2, ..., w_n) > P(Spam|w_1, w_2, ..., w_n)$, then the message is classified as ham.\n",
    "    - If $P(Ham|w_1, w_2, ..., w_n) < P(Spam|w_1, w_2, ..., w_n)$, then the message is classified as spam.\n",
    "    - If $P(Ham|w_1, w_2, ..., w_n) = P(Spam|w_1, w_2, ..., w_n)$, then the message is not classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(message, log=False):\n",
    "    '''\n",
    "    message: a string\n",
    "    '''\n",
    "    \n",
    "    message = re.sub('\\W', ' ', message).lower().split()\n",
    "    \n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in p_words_spam:\n",
    "            p_spam_given_message *= p_words_spam[word]\n",
    "            \n",
    "        if word in p_words_ham:\n",
    "            p_ham_given_message *= p_words_ham[word]\n",
    "    if log == True:\n",
    "        print('P(Spam|message):', p_spam_given_message)\n",
    "        print('P(Ham|message):', p_ham_given_message)\n",
    "    \n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'unclassified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 1.3481290211300841e-25\n",
      "P(Ham|message): 1.9368049028589875e-27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_classifier('WINNER!! This is the secret code to unlock the money: C3421.', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 2.4372375665888117e-25\n",
      "P(Ham|message): 3.687530435009238e-21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_classifier(\"Sounds good, Tom, then see u there\", log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Spam Filter's Accuracy\n",
    "\n",
    "Let's see how well the filter does on our test set, which has 1,114 messages.  \n",
    "We can use our classifier to create a new column in our test set according to its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS predicted\n",
       "0   ham          Later i guess. I needa do mcat study too.       ham\n",
       "1   ham             But i haf enuff space got like 4 mb...       ham\n",
       "2  spam  Had your mobile 10 mths? Update to latest Oran...      spam\n",
       "3   ham  All sounds good. Fingers . Makes it difficult ...       ham\n",
       "4   ham  All done, all handed in. Don't know if mega sh...       ham"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['predicted'] = test['SMS'].apply(naive_bayes_classifier)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll write a function to measure the accuracy of our spam filter to find out how well our spam filter does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9874326750448833"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['accurate'] = test['Label'] == test['predicted']\n",
    "accuracy = test['accurate'].sum()/len(test['accurate'])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is close to 98.74%, which is really good. Our spam filter looked at 1,114 messages that it hasn't seen in training, and classified 1,100 correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with sklearn implementation\n",
    "\n",
    "Several Naive Bayes algorithms are available within sklearn.  \n",
    "The one we hard-coded is the [Multinomial](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) and we are going to give sklearn's one a try to compare their performance.  \n",
    "We'll also leverage the Natural Language Processing library [NLTK](https://www.nltk.org) to clean our data with the following process:\n",
    "- Tokenize - split the sentence in a list of words\n",
    "- Remove punctuation and [stop-words](https://en.wikipedia.org/wiki/Stop_word)\n",
    "- [Lemmatize](https://en.wikipedia.org/wiki/Lemmatisation) - replace words with their 'root' (\"speaking\" would become \"speak\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_wordnet_pos(word): \n",
    "    ''' \n",
    "    Returns the context (Part-Of-Speech) for a word.\n",
    "    \n",
    "    Parameters:\n",
    "    word (string)\n",
    "    \n",
    "    Returns:\n",
    "    wordnet type (object)\n",
    "    '''\n",
    "    \n",
    "    tag = nltk.pos_tag([word])\n",
    "    tag_map = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    \n",
    "    return tag_map.get(tag[0][1][0], wordnet.NOUN)\n",
    "\n",
    "def tokenize_lemmatize(sentence):\n",
    "    '''\n",
    "    Tokenize, clean (punctuation, stop-words) and lemmatize list of strings\n",
    "    \n",
    "    Parameters:\n",
    "    sentence (list): list of words\n",
    "    \n",
    "    Returns:\n",
    "    cleaned list (list)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    return [wnl.lemmatize(word, get_wordnet_pos(word)).lower()\n",
    "            for word in tokens\n",
    "            if word not in stop_words\n",
    "           ]\n",
    "\n",
    "# Parameters initialization \n",
    "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Clean the sms\n",
    "sms['SMS'] = sms['SMS'].apply(tokenize_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn is bundled with a tool to build the word occurence matrix: [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)  \n",
    "Let's use it and append the result to our dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>...</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>èn</th>\n",
       "      <th>é</th>\n",
       "      <th>ú1</th>\n",
       "      <th>ü</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[nah, i, think, go, usf, life, around, though]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>[this, 2nd, time, try, 2, contact, u, u, 750, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>[will, ü, b, go, esplanade, fr, home]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>[pity, mood, so, suggestion]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>[the, guy, bitching, i, act, like, interested,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>[rofl, its, true, name]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 7960 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS  0  00  000  \\\n",
       "0      ham  [go, jurong, point, crazy, available, bugis, n...  0   0    0   \n",
       "1      ham                     [ok, lar, joking, wif, u, oni]  0   0    0   \n",
       "2     spam  [free, entry, 2, wkly, comp, win, fa, cup, fin...  0   0    0   \n",
       "3      ham      [u, dun, say, early, hor, u, c, already, say]  0   0    0   \n",
       "4      ham     [nah, i, think, go, usf, life, around, though]  0   0    0   \n",
       "...    ...                                                ... ..  ..  ...   \n",
       "5567  spam  [this, 2nd, time, try, 2, contact, u, u, 750, ...  0   0    0   \n",
       "5568   ham              [will, ü, b, go, esplanade, fr, home]  0   0    0   \n",
       "5569   ham                       [pity, mood, so, suggestion]  0   0    0   \n",
       "5570   ham  [the, guy, bitching, i, act, like, interested,...  0   0    0   \n",
       "5571   ham                            [rofl, its, true, name]  0   0    0   \n",
       "\n",
       "      000pes  008704050406  0089  0121  01223585236  ...  zogtorius  zoom  \\\n",
       "0          0             0     0     0            0  ...          0     0   \n",
       "1          0             0     0     0            0  ...          0     0   \n",
       "2          0             0     0     0            0  ...          0     0   \n",
       "3          0             0     0     0            0  ...          0     0   \n",
       "4          0             0     0     0            0  ...          0     0   \n",
       "...      ...           ...   ...   ...          ...  ...        ...   ...   \n",
       "5567       0             0     0     0            0  ...          0     0   \n",
       "5568       0             0     0     0            0  ...          0     0   \n",
       "5569       0             0     0     0            0  ...          0     0   \n",
       "5570       0             0     0     0            0  ...          0     0   \n",
       "5571       0             0     0     0            0  ...          0     0   \n",
       "\n",
       "      zouk  zyada  èn  é  ú1  ü  〨ud  鈥  \n",
       "0        0      0   0  0   0  0    0  0  \n",
       "1        0      0   0  0   0  0    0  0  \n",
       "2        0      0   0  0   0  0    0  0  \n",
       "3        0      0   0  0   0  0    0  0  \n",
       "4        0      0   0  0   0  0    0  0  \n",
       "...    ...    ...  .. ..  .. ..  ... ..  \n",
       "5567     0      0   0  0   0  0    0  0  \n",
       "5568     0      0   0  0   0  1    0  0  \n",
       "5569     0      0   0  0   0  0    0  0  \n",
       "5570     0      0   0  0   0  0    0  0  \n",
       "5571     0      0   0  0   0  0    0  0  \n",
       "\n",
       "[5572 rows x 7960 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer= lambda x:x)\n",
    "vector = vectorizer.fit_transform(sms['SMS'])\n",
    "\n",
    "vocabulary_ = vectorizer.get_feature_names()\n",
    "word_count_ = pd.DataFrame(vector.toarray(), columns=vocabulary_)\n",
    "sms_clean = pd.concat([sms, word_count_], axis=1, join='outer')\n",
    "\n",
    "sms_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use [sklearn's algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).  \n",
    "This time, we'll cross-validate our result (hence the cleaning of the entire sms dataset in the first place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X = sms_clean.iloc[:, 2:]\n",
    "y = sms_clean[['Label']].values.ravel()\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "kf = KFold(n_splits=10, random_state=96121, shuffle=True)\n",
    "\n",
    "# labels inverse the order of the matrix to comply with standards\n",
    "y_pred = cross_val_predict(mnb, X, y, cv=kf)\n",
    "conf_mat = confusion_matrix(y, y_pred, labels=['spam','ham'])\n",
    "\n",
    "tp, fn, fp, tn = conf_mat.ravel()\n",
    "fnr = fn / (tp+fn)\n",
    "fpr = fp / (fp+tn)\n",
    "acc = (tp+tn) / (tp+tn+fp+fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than solely focusing on accuracy, we may compute the confusion matrix to better understand our algorithm efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam</th>\n",
       "      <th>Ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Classified spam</th>\n",
       "      <td>712</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classified ham</th>\n",
       "      <td>62</td>\n",
       "      <td>4763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Spam   Ham\n",
       "Classified spam   712    35\n",
       "Classified ham     62  4763"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat_ = pd.DataFrame(conf_mat, columns=['Spam', 'Ham'], index=['Classified spam', 'Classified ham'])\n",
    "conf_mat_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35 were classified as spam while they were actually ham and 64 were contrarily mistaken.  \n",
    "From that matrix, we can compute two extra metrics:\n",
    "\n",
    "$ \\text{Miss rate (False Negative rate)} = \\frac{FN}{TP+FN} $\n",
    "\n",
    "$ \\text{Fall-Out (False Positive rate)} = \\frac{FP}{FP+TN} $ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>98.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miss rate</th>\n",
       "      <td>4.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fall-Out</th>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Score (%)\n",
       "Accuracy       98.26\n",
       "Miss rate       4.69\n",
       "Fall-Out        1.28"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = pd.DataFrame([acc*100, fnr*100, fpr*100], columns=['Score (%)'], index=['Accuracy', 'Miss rate', 'Fall-Out']).round(2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Positives (messages classified as spam while being actual ham) are much more harmful than False Negatives.  \n",
    "If we were to apply such a filter on messages sent on the network, it would mean that a fraction of texts wouldn't reach their recipients, as opposed to the miss-rate which would send actual spam to recipients.  \n",
    "Yet, the accuracy of our model is great: 98.26% of all messages are correctly classified.  \n",
    "Though it is lower than our implementation of Naive Bayes, it is due to the cross-validation previously used, which was skipped in the first step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we managed to build a spam filter for SMS messages using the multinomial Naive Bayes algorithm.  \n",
    "Our handcrafted Naive Bayes doesn't perform badly at all compared to the one from sklearn.\n",
    "\n",
    "Another interesting metric is the execution time.  \n",
    "In order not to overload the notebook with several time execution logs, the following timings (in second) were measured separately:\n",
    "\n",
    "|                   | Handcrafted | Sklearn |\n",
    "|:------------------|-------------|---------|\n",
    "| Data cleaning     | 0.1         | 45.9    |\n",
    "| Word-count matrix | 36.4        | 0.6     |\n",
    "| NB algorithm      | 15.6        | 13.7    |\n",
    "\n",
    "\n",
    "The data-cleaning being obviously more thorough (with lemmatization) in our modern worflow, it explains the timing difference.  \n",
    "The matrix calculation was suboptimal on our version and Sklearn CountVectorizer is faster by several orders of magnitude.  \n",
    "Lastly, the algorithm itself is not shy compared to optimized computation of sklearn. \n",
    "\n",
    "The filter have an accuracy of 98.25%, which is a pretty good result. Our initial goal was an accuracy of over 90%, and we managed to do way better than that. Lowering the Fall-Out would be the next step in enhancing it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
